{
 "cells": [
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# **\"Gemini Clone (With Memory)\"**"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### The purpose of this notebook is to make Gemini ChatBot Clone with Memory.\n",
    "\n",
    "#### **Libraries Used:**\n",
    "- gradio\n",
    "- langchain\n",
    "- GenerativeAI\n",
    "\n",
    "#### **Work Flow:**\n",
    "- Load model using API key\n",
    "- Initialize the model with desired temperature\n",
    "- Define the memory (Here I'm using ConversationTokenBufferMemory with max_token_limit, you can adjust the value of token limit as per your requirement)\n",
    "- Define the conversational chain with memory\n",
    "- Define the prompt template\n",
    "- Set up Gradio interface\n",
    "- Run the interface\n"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# +_+_+_+_+_+_+_+_+_+_+_+_+_+_+_+_+_+_+_+_+_+_+_+_+_+_+_+_+_+_+_+_+_+_+_+_+_+_+_+_+_+_+_+_+"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### 1. Initialize the libraries"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "import gradio as gr\n",
    "from langchain.chains import ConversationChain\n",
    "from langchain.memory import ConversationTokenBufferMemory\n",
    "from langchain.prompts import PromptTemplate\n",
    "from google.generativeai import client\n",
    "import google.generativeai as genai\n",
    "from langchain_google_genai import ChatGoogleGenerativeAI"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### 2. Initialize the model"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Set up your Google API key\n",
    "client.configure(api_key='YOUR_API_KEY')\n",
    "\n",
    "# Initialize the model with desired temperature\n",
    "model = ChatGoogleGenerativeAI(model=\"gemini-pro\", temperature=0.7)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### 3. Define the memory \n",
    "##### ConversationTokenBufferMemory is used to store the conversation history. The tokens in the memory specify the number of characters to keep in the memory as per required. If the token limit is exceeded, the oldest tokens are deleted and the bot keeps track of only the most recent chat hence improves the accuracy of the bot with the ongoing conversation."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "memory = ConversationTokenBufferMemory(llm=model, max_token_limit=1000)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### 4. Define the prompt template"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "prompt_template = \"\"\"\n",
    "The following is a conversation with an AI assistant. The assistant is helpful, creative, clever, supportive and very friendly.\n",
    "\n",
    "{history}\n",
    "\n",
    "Human: {input}\n",
    "AI:\n",
    "\"\"\""
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### 5. Define the conversational chain"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "chatbot_chain = ConversationChain(\n",
    "    llm=model,\n",
    "    memory=memory,\n",
    "    prompt=PromptTemplate(template=prompt_template, input_variables=[\"history\", \"input\"])\n",
    ")"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### 6. Generate the response"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "def respond_to_user(user_input, history=None):\n",
    "    if history is None:\n",
    "        history = []\n",
    "\n",
    "    # Generate response\n",
    "    response = chatbot_chain.run(input=user_input)\n",
    "    history.append((user_input, response))\n",
    "    return response, history"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### 7. Host the ChatBot on Gradio"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 1,
   "metadata": {},
   "outputs": [
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "c:\\Users\\PMLS\\anaconda3\\envs\\traning_env\\lib\\site-packages\\langchain_core\\_api\\deprecation.py:139: LangChainDeprecationWarning: The class `ConversationChain` was deprecated in LangChain 0.2.7 and will be removed in 1.0. Use RunnableWithMessageHistory: https://api.python.langchain.com/en/latest/runnables/langchain_core.runnables.history.RunnableWithMessageHistory.html instead.\n",
      "  warn_deprecated(\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Running on local URL:  http://127.0.0.1:7860\n",
      "Running on public URL: https://e3e8d725a5056d92b7.gradio.live\n",
      "\n",
      "This share link expires in 72 hours. For free permanent hosting and GPU upgrades, run `gradio deploy` from Terminal to deploy to Spaces (https://huggingface.co/spaces)\n"
     ]
    },
    {
     "data": {
      "text/html": [
       "<div><iframe src=\"https://e3e8d725a5056d92b7.gradio.live\" width=\"100%\" height=\"500\" allow=\"autoplay; camera; microphone; clipboard-read; clipboard-write;\" frameborder=\"0\" allowfullscreen></iframe></div>"
      ],
      "text/plain": [
       "<IPython.core.display.HTML object>"
      ]
     },
     "metadata": {},
     "output_type": "display_data"
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "c:\\Users\\PMLS\\anaconda3\\envs\\traning_env\\lib\\site-packages\\langchain_core\\_api\\deprecation.py:139: LangChainDeprecationWarning: The method `Chain.run` was deprecated in langchain 0.1.0 and will be removed in 0.3.0. Use invoke instead.\n",
      "  warn_deprecated(\n"
     ]
    }
   ],
   "source": [
    "# Create the Gradio Blocks interface\n",
    "with gr.Blocks() as demo:\n",
    "    # Title and description centered\n",
    "    gr.Markdown(\n",
    "        \"\"\"\n",
    "        <div style=\"text-align: center;\">\n",
    "            <h1>GEMINI CLONE ðŸ¤–ðŸŽ€</h1>\n",
    "            <p>Ask me anything! ðŸ’¡</p>\n",
    "        </div>\n",
    "        \"\"\",\n",
    "        elem_id=\"title-description\"\n",
    "    )\n",
    "    # Input and buttons\n",
    "    with gr.Row():\n",
    "        with gr.Column():\n",
    "            user_input = gr.Textbox(label=\"You\", lines=5, placeholder=\"Type your message here...\")\n",
    "            \n",
    "            with gr.Row():\n",
    "                submit_btn = gr.Button(\"Submit\")\n",
    "                clear_btn = gr.Button(\"Clear\")\n",
    "    \n",
    "    # Output\n",
    "    bot_output = gr.Textbox(label=\"Bot\", lines=5, placeholder=\"The chatbot's response will appear here...\")\n",
    "    \n",
    "    # Define interactions\n",
    "    def on_submit(user_input, history):\n",
    "        response, updated_history = respond_to_user(user_input, history)\n",
    "        return response, updated_history\n",
    "    \n",
    "    # Set up event handlers\n",
    "    submit_btn.click(on_submit, inputs=[user_input, gr.State()], outputs=[bot_output, gr.State()])\n",
    "    clear_btn.click(lambda: (\"\", []), outputs=[user_input, gr.State()])\n",
    "\n",
    "# Launch the interface\n",
    "if __name__ == \"__main__\":\n",
    "    demo.launch(share=True)"
   ]
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "traning_env",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.9.19"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 2
}
